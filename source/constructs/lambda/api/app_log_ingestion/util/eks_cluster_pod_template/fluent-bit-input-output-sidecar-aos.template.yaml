    [INPUT]
        Name                tail
        Tag                 $LOGHUB_CONFIG_TAG
        Path                $LOGHUB_CONFIG_PATH
        Path_Key            file_name
        DB                  $LOGHUB_CHECKPOINT
        DB.locking          True    
        Multiline           $LOGHUB_MULTILINE_SWITCH
        $LOGHUB_PARSER
        Mem_Buf_Limit       10M
        # Since "Skip_Long_Lines" is set to "On", be sure to adjust the "Buffer_Chunk_Size","Buffer_Max_Size" according to the actual log. If the parameters are adjusted too much, the number of duplicate records will increase. If the value is too small, data will be lost. 
        # https://docs.fluentbit.io/manual/pipeline/inputs/tail
        Buffer_Chunk_Size   512k
        Buffer_Max_Size     5M
        Skip_Long_Lines     On
        Skip_Empty_Lines    On
        Storage.type        filesystem
        Read_from_Head      True

    [OUTPUT]
        Name                opensearch
        Match               $LOGHUB_CONFIG_TAG
        AWS_Region          $LOGHUB_CONFIG_REGION
        Host                $LOGHUB_AOS_ENDPOINT
        Port                443
        Retry_Limit         False
        AWS_Auth            On
        TLS                 On
        Buffer_Size         20M
        #When enabled, generate _id for outgoing records. This prevents duplicate records when retrying.
        Generate_ID         On
        Logstash_Format     On
        Logstash_Prefix     $LOGHUB_AOS_IDX_PREFIX
        Logstash_DateFormat %Y%m%d
        Time_Key            time
        Time_Key_Nanos      Off
        Write_Operation     create
        $LOGHUB_EC2_ROLE_ARN
        Workers             1
